{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ATARI Pong DQN — Train or Load a Checkpoint in Google Colab\n",
    "This notebook lets you:\n",
    "- Train a DQN agent on Pong from scratch or resume from a checkpoint.\n",
    "- Alternatively, load an existing checkpoint and run an evaluation with inline rendering.\n",
    "\n",
    "It assumes the Python files from this repo are available in the same directory (nn.py, helper.py, rpbuf.py, checkpoint.py, config.py, etc.). If using Google Colab, upload the project folder or mount Google Drive and cd into the folder that contains these files.\n"
   ],
   "id": "79b6ed7df59f98de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1) Optional: Mount Google Drive (Colab)\n",
    "If your checkpoints or project folder are on Drive, mount it here.\n"
   ],
   "id": "b046a6edab8e3c4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# In Colab, uncomment to mount Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/path/to/ATARI\n"
   ],
   "id": "b0093fe458438cee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2) Install dependencies (Colab)\n",
    "If running locally with dependencies already installed, you can skip this.\n"
   ],
   "id": "7dc8647c095c5708"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, sys, subprocess\n",
    "IN_COLAB = 'COLAB_RELEASE_TAG' in os.environ or 'COLAB_GPU' in os.environ\n",
    "if IN_COLAB:\n",
    "    print('Installing packages for Colab...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                           'gymnasium[atari,accept-rom-license]>=0.29.1',\n",
    "                           'ale-py', 'autorom', 'opencv-python', 'pillow'])\n",
    "    # Download Atari ROMs (required by ALE); accepts license automatically\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'AutoROM', '--accept-license'])\n",
    "    except Exception as e:\n",
    "        print('AutoROM download step failed or already satisfied:', e)\n"
   ],
   "id": "dd6828fbdd36dd26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3) Imports and setup\n",
   "id": "ddf1a61e626251c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import ale_py  # ensure ALE is registered\n",
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "\n",
    "from nn import DQN\n",
    "from helper import preprocess_obs, init_state_stack, get_state_from_stack\n",
    "from rpbuf import ReplayBuffer\n",
    "from checkpoint import save_checkpoint, load_checkpoint\n",
    "from config import DEVICE, VALID_ACTIONS, CHECKPOINT_DIR\n",
    "\n",
    "device = DEVICE\n",
    "print('Using device:', device)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ],
   "id": "c3c18eb20ac6057b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Configuration\n",
    "Adjust training hyperparameters, mode, and checkpoint behavior here.\n"
   ],
   "id": "188b4146c801569a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mode: 'train' to train/resume, 'eval' to just run episodes with a loaded policy\n",
    "MODE = 'train'  # 'train' or 'eval'\n",
    "\n",
    "# Checkpoint loading option at startup:\n",
    "#   'none'   -> start fresh\n",
    "#   'latest' -> load newest checkpoint in models/\n",
    "#   path     -> a specific checkpoint file path (e.g., 'models/dqn_pong_model_100000.pth')\n",
    "CHECKPOINT_OPTION = 'none'\n",
    "\n",
    "# Evaluation episodes (used if MODE == 'eval')\n",
    "EVAL_EPISODES = 3\n",
    "\n",
    "# Training hyperparameters (defaults are modest for Colab)\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY_FRAMES = 1_000_000\n",
    "TARGET_UPDATE = 10_000\n",
    "REPLAY_INIT = 5_000   # a smaller initial buffer fill for Colab\n",
    "REPLAY_CAP = 100_000  # smaller buffer to fit memory constraints\n",
    "MAX_FRAMES = 200_000  # adjust for Colab session length\n",
    "MOVE_PENALTY = 0.0    # optional movement penalty shaping (0 is fine)\n"
   ],
   "id": "4dd7f11ed686bbe5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5) Utilities for training and evaluation\n",
   "id": "98b43123a5db681c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_env(render_mode=None):\n",
    "    return gym.make('ALE/Pong-v5', render_mode=render_mode)\n",
    "\n",
    "\n",
    "def select_action(policy_net, state, frame_idx):\n",
    "    eps = max(EPS_END, EPS_START - (EPS_START - EPS_END) * frame_idx / EPS_DECAY_FRAMES)\n",
    "    if np.random.rand() < eps:\n",
    "        a_idx = np.random.randint(len(VALID_ACTIONS))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            s = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "            s = s.float() / 255.0\n",
    "            q_values = policy_net(s)\n",
    "            a_idx = int(torch.argmax(q_values, dim=1).item())\n",
    "    return a_idx, eps\n",
    "\n",
    "\n",
    "def optimize_model(policy_net, target_net, optimizer, replay):\n",
    "    if len(replay) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay.sample(BATCH_SIZE)\n",
    "\n",
    "    states = torch.from_numpy(states).float().to(device) / 255.0\n",
    "    next_states = torch.from_numpy(next_states).float().to(device) / 255.0\n",
    "    actions     = torch.from_numpy(actions).long().to(device)\n",
    "    rewards     = torch.from_numpy(rewards).to(device)\n",
    "    dones       = torch.from_numpy(dones).to(device)\n",
    "\n",
    "    q_values = policy_net(states)\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states).max(1)[0]\n",
    "        expected_q = rewards + GAMMA * next_q_values * (1.0 - dones)\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(q_value, expected_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def play_inline_frame(frame_array, scale=2):\n",
    "    # frame_array is RGB (H,W,3); upscale for visibility\n",
    "    img = Image.fromarray(frame_array)\n",
    "    if scale != 1:\n",
    "        img = img.resize((img.width * scale, img.height * scale))\n",
    "    clear_output(wait=True)\n",
    "    display(img)\n"
   ],
   "id": "a09cd6a60f1d8399"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6) Train function (supports resume via checkpoint)\n",
   "id": "e64086aa43d68dbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train():\n",
    "    env = make_env(render_mode=None)\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Networks & optimizer\n",
    "    policy_net = DQN(in_channels=4, num_actions=len(VALID_ACTIONS)).to(device)\n",
    "    target_net = DQN(in_channels=4, num_actions=len(VALID_ACTIONS)).to(device)\n",
    "    optimizer = torch.optim.Adam(policy_net.parameters(), lr=LR)\n",
    "\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    # Replay buffer\n",
    "    replay = ReplayBuffer(cap=REPLAY_CAP)\n",
    "\n",
    "    # Try to resume from checkpoint\n",
    "    start_frame = load_checkpoint(\n",
    "        policy_net,\n",
    "        device,\n",
    "        CHECKPOINT_OPTION,\n",
    "        target_net=target_net,\n",
    "        optimizer=optimizer,\n",
    "        directory=CHECKPOINT_DIR,\n",
    "    )\n",
    "\n",
    "    # Initialize state stack\n",
    "    stack = init_state_stack(obs)\n",
    "    state = get_state_from_stack(stack)\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_move_count = 0\n",
    "\n",
    "    for frame_idx in range(start_frame + 1, MAX_FRAMES + 1):\n",
    "        a_idx, eps = select_action(policy_net, state, frame_idx)\n",
    "        if frame_idx % 1000 == 0:\n",
    "            print(f'Frame {frame_idx}, epsilon: {eps:.3f}')\n",
    "\n",
    "        # Save checkpoint periodically\n",
    "        if frame_idx % 10000 == 0:\n",
    "            save_checkpoint(policy_net, target_net, optimizer, frame_idx, directory=CHECKPOINT_DIR)\n",
    "\n",
    "        env_action = VALID_ACTIONS[a_idx]\n",
    "        next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        is_movement = (env_action != 0)\n",
    "        movement_reward = MOVE_PENALTY if is_movement else 0.0\n",
    "        episode_move_count += is_movement\n",
    "        shaped_reward = reward + movement_reward\n",
    "\n",
    "        frame = preprocess_obs(next_obs)\n",
    "        stack.append(frame)\n",
    "        next_state = get_state_from_stack(stack)\n",
    "\n",
    "        replay.push(state, a_idx, shaped_reward, next_state, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        if frame_idx > REPLAY_INIT:\n",
    "            optimize_model(policy_net, target_net, optimizer, replay)\n",
    "\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if done:\n",
    "            print(\n",
    "                f'Frame {frame_idx}, episode reward: {episode_reward:.1f}, '\n",
    "                f'episode move count: {episode_move_count}, epsilon: {eps:.3f}'\n",
    "            )\n",
    "            obs, info = env.reset()\n",
    "            stack = init_state_stack(obs)\n",
    "            state = get_state_from_stack(stack)\n",
    "            episode_reward = 0.0\n",
    "            episode_move_count = 0\n",
    "\n",
    "    # Final save at the end of training loop\n",
    "    save_checkpoint(policy_net, target_net, optimizer, MAX_FRAMES, directory=CHECKPOINT_DIR)\n",
    "    env.close()\n",
    "\n",
    "    print('Training complete. Checkpoints in:', CHECKPOINT_DIR)\n"
   ],
   "id": "dd0cc0f22055c8a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7) Evaluate function (inline rendering)\n",
   "id": "d416ee86526f20b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(checkpoint_option: str, episodes: int = 3, fps: int = 30, render_scale: int = 2):\n",
    "    env = make_env(render_mode='rgb_array')\n",
    "\n",
    "    policy_net = DQN(in_channels=4, num_actions=len(VALID_ACTIONS)).to(device)\n",
    "    policy_net.eval()\n",
    "\n",
    "    _ = load_checkpoint(policy_net, device, checkpoint_option, directory=CHECKPOINT_DIR)\n",
    "\n",
    "    all_rewards = []\n",
    "    frame_delay = 1.0 / max(1, fps)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        stack = init_state_stack(obs)\n",
    "        state = get_state_from_stack(stack)\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        while not done:\n",
    "            # render current env frame\n",
    "            frame_rgb = env.render()  # (H,W,3) RGB\n",
    "            if frame_rgb is not None:\n",
    "                play_inline_frame(frame_rgb, scale=render_scale)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                s = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "                if s.dtype != torch.float32:\n",
    "                    s = s.float() / 255.0\n",
    "                q_values = policy_net(s)\n",
    "                a_idx = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "            env_action = VALID_ACTIONS[a_idx]\n",
    "            next_obs, reward, terminated, truncated, info = env.step(env_action)\n",
    "            done = terminated or truncated\n",
    "            ep_reward += reward\n",
    "\n",
    "            frame = preprocess_obs(next_obs)\n",
    "            stack.append(frame)\n",
    "            state = get_state_from_stack(stack)\n",
    "\n",
    "            if fps > 0:\n",
    "                time.sleep(frame_delay)\n",
    "\n",
    "        all_rewards.append(ep_reward)\n",
    "        print(f'[Eval] Episode {ep + 1}/{episodes} reward: {ep_reward:.1f}')\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    if all_rewards:\n",
    "        mean_r = np.mean(all_rewards)\n",
    "        std_r = np.std(all_rewards)\n",
    "        print(f'[Eval] Mean reward over {episodes} episodes: {mean_r:.2f} ± {std_r:.2f}')\n"
   ],
   "id": "f4f5b617f3441fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8) Run (choose mode in the config cell)\n",
   "id": "6abe23aa6c350928"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if MODE == 'train':\n",
    "    print('Starting training...')\n",
    "    train()\n",
    "elif MODE == 'eval':\n",
    "    print('Starting evaluation...')\n",
    "    evaluate(CHECKPOINT_OPTION, episodes=EVAL_EPISODES)\n",
    "else:\n",
    "    raise ValueError(\"MODE must be 'train' or 'eval'\")"
   ],
   "id": "a391a80932ef8b56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
